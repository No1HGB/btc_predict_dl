# BTCUSDT NextDay Predict using CNN

### (Result: Fail)

### Colab Codes: https://drive.google.com/drive/folders/1s86vzUAwXBJrTmZ8PBfZUw5JMMBQ5h2d?usp=sharing

## Background

- 약 한달 간 기술적 분석(거래량, 봉, 이평선 등 과거 데이터 활용하는 방식)을 통해 만든 알고리즘 선물 투자가 %로 따졌을 때 꽤나 괜찮은 수익을 안겨주면서 과거 데이터 기반한 투자가 가능하겠다는 생각.
  => 6/20 $729 => 7/26 $950 (오직 알고리즘에 의한 결과)

####

- 기술적 분석은 금융기관에서 펀드매니저들이 과거에 계속 하던 방식이고 지금도 다 하고 있음. 실제로 미국장의 80%(2024년) 정도는 이미 알고리즘에 의한 거래.
  => https://www.quora.com/What-percentage-of-trading-is-done-by-computers#:~:text=Approximately%2050%2D60%25%20of%20trading,number%20between%2050%2D60%25.

####

--------

## 단기투자 기본 지식

- 장기적 투자는 매크로(거시경제: 금리, 펀더멘탈(PER, PBR 등))에 의해 결정남

####

- 그러나, 단기적 투자는 심리에 의해 결정됨

####

- 단기적 관점에서 왜 상승 모멘텀, 하락 모멘텀이 존재하나? => 사람들은 군중심리에 의해 가격이 상승할 때 사려는 경향을 보이고, 내려갈 때 팔고 싶은 경향을 보임

####

- 단기적 관점에서 가격은 랜덤이며 예측은 불가능하다는 '효율적 시장 가설 이론'은 맞지 않음

####

- 만약 가격이 정말 랜덤이라면 상승 모멘텀과 하락 모멘텀이 존재해서는 안됨. 그러나 현실에선 상승, 하락, 횡보가 번갈아가며 나타남.

--------

## 기본 가정

- 가정1: 투자자들의 '큰 심리'는 인간의 심리이므로 비슷하다

####

- 가정2: 투자자들의 '작은 심리(디테일)'는 조금씩 바뀐다(경험상 약 1달-1달 반 사이)
  => 얼마만큼의 거래량이 큰 거래량인지, 얼마만큼의 가격 변화율이 큰 변화율인지 등

####

- 가정3: 단기 투자자들은 n일 동안의 데이터를 보고 투자를 한다
  => 즉, n일 동안의 데이터로 그 다음날의 데이터를 대략적으로 알 수 있다.

####

- 따라서 최근 데이터에 조금 더 가중치를 주고, 투자를 결정하는 데이터인 'n일 동안'의 n을 조절하면 그 다음날의 데이터를 대략적으로 알 수 있다.

####

- 하지만 90% 예측 이딴 예측은 불가능. 외부 요인에도 크게 영향을 받기 때문.
- 예를 들어, 바이든 사퇴 뉴스가 뜬 후 하락빔이 뜨는 것 등은 절대 예측 불가능.
- 그러나, (어떤 이유에서든지) 하락빔/상승빔이 발생한 이후 투자자들의 심리는 예측이 가능 => 수식(가중치 행렬)으로 표현 가능

####

- 극단적으로 말해서, 상승 또는 하락만 맞추면 그만인 게임이고 기본 50% 확률임
- 목표: 60%

####

--------

## 데이터 설명

- 전체 데이터 약 49만개. 전처리 완료.
- 5분봉을 기준으로, x_data는 3일간(n=3)의 데이터 y_data는 그 다음날 하루치 데이터 (n값은 조정 가능)
- x_data 개수: 12*24*3=864
- y_data 개수: 12*24*1=288

#### 전처리 전 기본 데이터

- 5분봉 시가(시작가격), 고가(최고가격), 저가(최저가격), 종가(마감가격), 거래량
- EMA50, EMA200: 종가의 50일 지수이동평균선, 종가의 200일 지수이동평균선
- volume_MA: 거래량의 50일 이동평균선

#### 전처리 후 데이터

- up_delta: 고가/max(종가,시가) => 윗꼬리
- delta: 종가/시가 => 몸통
- down_delta: 저가/min(종가,시가) => 아랫꼬리
- volume_ratio: 거래량 / volume_MA(거래량의 50일 이동평균선) => 거래량의 50일 평균 대비 상대적인 크기
- volume_delta: 거래량 / 전 봉 거래량 => 전 봉 거래량 대비 현재 봉 거래량의 상대적인 크기
- distance_50: 종가 / 종가의 50일 지수이동평균선 => 50 지수이평선 대비 종가가 떨어진 거리
- distance_200: 종가 / 종가의 200일 지수이동평균선 => 200 지수이평선 대비 종가가 떨어진 거리

#### 왜 50, 200일인 이유

- 수학적으로 의미가 있어서가 아닌, 대부분의 트레이더들이 참고하는 지표이기 때문
- 실제로 가격 데이터를 보면 50일선과 200일선이 저항선의 역할을 하는 경우가 많으며,
  만약 봉이 50일선을 뚫은 경우 해당 방향으로 추세가 강력히 지속이 되는 경우가 많다.
- 모두가 지지,저항으로 생각하면 해당 가격이 지지/저항으로 된다.

--------

## 모델 설명

- 실제로 논문도 찾아보고 모델도 돌려봄 => papers 참고
- 투자자들은 차트를 보고 심리 상태에 의해 매매를 한다 => 차트를 이미지 형태로 보자
  => x_data(n일)와 y_data(그 다음날) 사이의 이미지 변환으로 보자

다음 표를 보자

    |   | volume_ratio | down_delta | delta | up_delta |
    |---|--------------|------------|-------|----------|
    | 0 | 0.8          | 0.97       | 1.01  | 1.01     |
    | 1 | 2.1          | 0.99       | 1.02  | 1.02     |
    | 2 | 3            | 0.98       | 0.98  | 1.04     |

- 표는 x_data의 일부를 나타낸 것임. 0,1,2.. 데이터는 시간순 데이터이며, 행렬의 크기는 (n * 12 * 24, 4)임.

#### 즉, n일간의 5분봉 차트를 시계방향으로 90도 회전시킨것으로 볼 수 있음.

따라서 n일간의 5분봉 데이터가 하나의 차트 상 이미지로 나타내어진 것임.
이러한 이미지가 수없이 많이 있는 형식이 x_data와 y_data임.

- y_data도 마찬가지로 나타낼 수 있음.
- 현재 모델은 X: (numbers_of_data, n * 12 * 24, 4) => Y: (numbers_of_data, 1 * 12 * 24, 3)
  이미지 변환 모델임. (Y데이터의 경우 volume_ratio를 제외한 나머지 값들)

#### n일간의 5분봉 차트 이미지가 하루치의 5분봉 차트 이미지로 변환된다는 아이디어 => 2차원 합성곱 신경망(Conv2D)

##### 입력층

(numbers_of_data, n * 12 * 24, 4, 1) 입력

#### 첫 번째 합성곱층

패딩을 적용시켜 같은 크기로 이미지 변환

#### 풀링층(MaxPool)

행의 크기를 줄임. n일차의 데이터를 1일차의 데이터로 크기 줄임.
실제 투자자들이 큰 거래량, 큰 봉에 큰 영향을 받으므로 MaxPool 사용

#### 두 번째 합성곱층

패딩을 적용시켜 같은 크기로 이미지 변환

#### 출력층

열의 크기를 y열 크기에 맞춰 줄임(결과값에는 volume_ratio가 필요 없으므로)

### 하이퍼파라미터

- 약 10000개의 데이터로 필터크기, 커널 크기, 활성 함수 등을 최적화한 결과값이 현재 모델임
- 만약 전체 데이터에 맞는 하이퍼파라미터 최적화를 원한다면 hyperparam.py 참고(랜덤 서치, 그리드 서치 뭐 아무거나 사용해서 최적화 하면 됨)

### 가중치 행렬

- 투자자들의 '작은 심리'가 약 1달-1달반 사이에서 변하므로 12 * 24 * 45 = 12960 ~= 12000개의
  데이터에 가중치 행렬을 통해 가중치를 부여했음
- 48만개의 데이터와 최근 12000개의 데이터 간 가중치를 2:1로 설정
- 48만개 * 1.0 : 12000 * x = 2 : 1 계산식 사용하여 계산

### 스케일링

스케일링은 사용하지 않음.

- 가격 데이터는 정규분포가 아니므로 Standard 분포 스케일링 불가능.
- MinMaxScaling은 전체 데이터에 대해서, 또 각 기간(n일) 열 별로도 직접 구현하여 해보았으나 정확도가 더 떨어짐
- 사실 이미 주식 데이터 자체는 전처리를 통해서 적정 스케일이 되어있는 결과

### 기타 다른 모델

- 순차모델 사용해서 회귀로 풀어봤으나 2차원 합성곱이 더 잘 맞음
- RNN(LSTM,GRU): 계산량은 많고 잘 맞지도 않음. 실제로 타 논문들에서도 정확도가 더 떨어짐
- Transformer: 시도해보지 않음

-------

## 기본 설정

- tensorflow: 2.17.0
- CUDA:12.3,cuDNN: 8.9
- https://www.tensorflow.org/install/source?hl=en
- requirements.txt: 필수 패키지
- pip install tensorflow-gpu
